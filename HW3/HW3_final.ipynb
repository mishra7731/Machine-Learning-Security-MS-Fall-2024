{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1"
      ],
      "metadata": {
        "id": "cjRS0IOUjH4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq-PqYlqXz2_"
      },
      "outputs": [],
      "source": [
        "# Imports.\n",
        "import random\n",
        "import network as Network\n",
        "import mnist_loader as mnist_loader\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set the random seed. DO NOT CHANGE THIS!\n",
        "seedVal = 41\n",
        "random.seed(seedVal)\n",
        "np.random.seed(seedVal)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a pre-trained network. It has been saved as a pickle file. Load the model, and continue. The network has only one hidden layer of 30 units, 784 input units (MNIST images are  28√ó28=784  pixels large), and 10 output units. All the activations are sigmoidal."
      ],
      "metadata": {
        "id": "LlYlMbroh_ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model.\n",
        "with open('trained_network.pkl', 'rb') as f:\n",
        "    u = pickle._Unpickler(f)\n",
        "    u.encoding = 'latin1'\n",
        "    net = u.load()\n",
        "\n",
        "# Helpful function to load the MNIST data.\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
      ],
      "metadata": {
        "id": "nAfLggAEiCGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network is pretrained, so it should already be set up to predict characters. Run predict(n) to evaluate the  ùëõùë°‚Ñé  digit in the test set using the network. You should see that even this relatively simple network works really well (~97% accuracy). The output of the network is a one-hot vector indicating the network's predictions:"
      ],
      "metadata": {
        "id": "5BiqBWQgiIEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(n):\n",
        "    # Get the data from the test set\n",
        "    x = test_data[n][0]\n",
        "\n",
        "    # Print the prediction of the network\n",
        "    print('Network output: \\n' + str(np.round(net.feedforward(x), 2)) + '\\n')\n",
        "    print('Network prediction: ' + str(np.argmax(net.feedforward(x))) + '\\n')\n",
        "    print('Actual image: ')\n",
        "\n",
        "    # Draw the image\n",
        "    plt.imshow(x.reshape((28,28)), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Replace the argument with any number between 0 and 9999\n",
        "predict(8384)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "WQsCPO5wiMnj",
        "outputId": "61f9fe1d-732b-49a8-9404-2fa4e0e7a4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output: \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]]\n",
            "\n",
            "Network prediction: 8\n",
            "\n",
            "Actual image: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To actually generate adversarial examples we solve a minimization problem. We do this by setting a \"goal\" label called $ \\vec y_{goal} $ (for instance, if we wanted the network to think the adversarial image is an 8, then we would choose $ \\vec y_{goal} $ to be a one-hot vector with the eighth entry being 1). Now we define a cost function:\n",
        "\n",
        "$$ C = \\frac{1}{2} \\|\\vec y_{goal} - \\hat y(\\vec x)\\|^2_2 $$\n",
        "\n",
        "where $ \\| \\cdot \\|^2_2 $ is the squared Euclidean norm and $ \\hat y $ is the network's output. It is a function of $ \\vec x $, the input image to the network, so we write $ \\hat y(\\vec x) $. Our goal is to find an $ \\vec x $ such that $ C $ is minimized. Hopefully this makes sense, because if we find an image $ \\vec x $ that minimizes $ C $ then that means the output of the network when given $ \\vec x $ is close to our desired output, $ \\vec y_{goal} $. So in full mathy language, our optimization problem is:\n",
        "\n",
        "$$ \\arg \\min_{\\vec x} C(\\vec x) $$\n",
        "\n",
        "that is, find the $ \\vec x $ that minimizes the cost $ C $.\n",
        "\n",
        "To actually do this we can do gradient descent on $ C $. Start with an initially random vector $ \\vec x $ and take steps (changing $ \\vec x $) gradually in the direction opposite of the gradient $ \\nabla_x C $. To actually get these derivatives we can perform backpropagation on the network. In contrast to training a network, where we perform gradient descent on the weights and biases, when we create adversarial examples we hold the weights and biases constant (because we don't want to change the network!), and change the inputs to our network."
      ],
      "metadata": {
        "id": "c_PiZ8HpiWu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to evaluate the non-linearity and it's derivative:"
      ],
      "metadata": {
        "id": "UzYcVS7kikpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "GngMlHCYisc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, a function to find the gradient derivatives of the cost function,  ‚àáùë•ùê∂  with respect to the input  ùë•‚Éó , with a goal label of  ùë¶‚Éóùëîùëúùëéùëô . (Don't worry too much about the implementation, just know it calculates derivatives)."
      ],
      "metadata": {
        "id": "lHTwX-lwiwU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_derivative(net, x, y):\n",
        "    \"\"\"\n",
        "    Calculating the gradient of the cost function with respect to the input.\n",
        "\n",
        "    Parameters:\n",
        "        net: The trained neural network.\n",
        "        x: The input image (original).\n",
        "        y_goal: The target output (one-hot vector of the desired class).\n",
        "\n",
        "    Returns:\n",
        "        gradient_x: The gradient of the cost function with respect to the input x.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" Calculate derivatives wrt the inputs\"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
        "\n",
        "\n",
        "    # feedforward\n",
        "    activation = x\n",
        "    activations = [x] # list to store all the activations, layer by layer\n",
        "    zs = [] # list to store all the z vectors, layer by layer\n",
        "    for b, w in zip(net.biases, net.weights):\n",
        "        z = np.dot(w, activation)+b\n",
        "        zs.append(z)\n",
        "        activation = sigmoid(z)\n",
        "        activations.append(activation)\n",
        "\n",
        "\n",
        "    # backward pass\n",
        "    delta = net.cost_derivative(activations[-1], y) * \\\n",
        "        sigmoid_prime(zs[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    # Update the gradients for the final layer\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    # Compute gradients for hidden layers\n",
        "    for l in range(2, net.num_layers):\n",
        "        z = zs[-l]\n",
        "        sp = sigmoid_prime(z)\n",
        "        delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "    # --- Return the gradient with respect to the input ---\n",
        "    gradient_x = np.dot(net.weights[0].transpose(), delta)\n",
        "    return gradient_x\n"
      ],
      "metadata": {
        "id": "5dWkNbbvi2yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual function that generates adversarial examples and a wrapper function:"
      ],
      "metadata": {
        "id": "vIwmQpFwi8Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (a) Non Targeted Attack"
      ],
      "metadata": {
        "id": "N38X9alDjCNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nonTargetedAdversarial(net, n, steps, eta):\n",
        "    \"\"\"\n",
        "    net : network object (neural network instance to use)\n",
        "    n : integer (our goal label (just an int, the function transforms it into a one-hot vector))\n",
        "    steps : integer (number of steps for gradient descent)\n",
        "    eta : float (step size for gradient descent)\n",
        "    \"\"\"\n",
        "\n",
        "    ####### Enter your code below #######\n",
        "     # Get the original image and its label\n",
        "    x, actual_label = test_data[n]\n",
        "\n",
        "    # Set the goal output\n",
        "    # One-hot encode the actual label (goal output)\n",
        "    y_actual = np.zeros((10, 1))\n",
        "    y_actual[actual_label] = 1\n",
        "\n",
        "    # Initialize x for adversarial gradient descent\n",
        "    adversarial_x = np.copy(x)\n",
        "\n",
        "    # Create a random image to initialize gradient descent with\n",
        "\n",
        "    # Gradient descent on the input\n",
        "    for _ in range(steps):\n",
        "        # Compute the derivative of the cost with respect to the input\n",
        "        gradient = input_derivative(net, adversarial_x, np.zeros((10, 1)))\n",
        "\n",
        "        # Perform the gradient descent update\n",
        "        adversarial_x += eta * np.sign(gradient)\n",
        "\n",
        "        # Clip the adversarial image to valid pixel intensity range [0, 1]\n",
        "        adversarial_x = np.clip(adversarial_x, 0, 1)\n",
        "\n",
        "    return adversarial_x"
      ],
      "metadata": {
        "id": "fslI5Z-LjEQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper function\n",
        "\n",
        "def generate(n, steps=100, eta=0.01):\n",
        "    \"\"\"\n",
        "    Wrapper function to generate and display a non-targeted adversarial example.\n",
        "\n",
        "    Parameters:\n",
        "        n : int\n",
        "            Index of the test example to attack.\n",
        "        steps : int\n",
        "            Number of gradient descent steps. Default is 100.\n",
        "        eta : float\n",
        "            Step size for gradient descent. Default is 0.01.\n",
        "    \"\"\"\n",
        "     ####### Enter your code below #######\n",
        "\n",
        "    # Find the vector x with the above function that you just wrote.\n",
        "\n",
        "    # Generate the adversarial example using non-targeted attack\n",
        "    adversarial_x = nonTargetedAdversarial(net, n, steps, eta)\n",
        "\n",
        "    # Pass the generated image (vector) to the neural network. Perform a forward pass, and get the prediction.\n",
        "    # Get the network's prediction on the adversarial example\n",
        "    adversarial_output = net.feedforward(adversarial_x)\n",
        "\n",
        "    # Display the network's output and prediction\n",
        "    print('Network Output: \\n' + str(np.round(adversarial_output, 2)) + '\\n')\n",
        "    print('Network Prediction: ' + str(np.argmax(adversarial_output)) + '\\n')\n",
        "\n",
        "    # Display the adversarial example\n",
        "    print('Adversarial Example: ')\n",
        "    plt.imshow(adversarial_x.reshape(28, 28), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QoFbKbQdjZ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's generate some adversarial examples! Use the function provided to mess around with the neural network. (For some inputs gradient descent doesn't always converge; 0 and 5 seem to work pretty well though. I suspect convergence is very highly dependent on our choice of random initial $ \\vec x $. We'll see later in the notebook if we force the adversarial example to \"look like\" a handwritten digit, convergence is much more likely. In a sense we will be adding regularization to our generation process)."
      ],
      "metadata": {
        "id": "i3I9VJz6jee9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "jOcin4qzjhjH",
        "outputId": "8af4ad1c-95b5-4c5a-c2d7-1a5468b8248d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Output: \n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "Network Prediction: 0\n",
            "\n",
            "Adversarial Example: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (b) Targeted Attack(s)"
      ],
      "metadata": {
        "id": "-ezSsyuXkI3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweet! We've just managed to create an image that looks utterly meaningless to a human, but the neural network thinks is a '5' with very high certainty. We can actually take this a bit further. Let's generate an image that looks like one number, but the neural network is certain is another. To do this we will modify our cost function a bit. Instead of just optimizing the input image, $ \\vec x $, to get a desired output label, we'll also optimize the input to look like a certain image, $ \\vec x_{target} $, at the same time. Our new cost function will be\n",
        "\n",
        "$$ C = \\|\\vec y_{goal} - y_{hat}(\\vec x)\\|^2_2 + \\lambda \\|\\vec x - \\vec x_{target}\\|^2_2 $$\n",
        "\n",
        "The added term tells us the distance from our $ \\vec x $ and some $ \\vec x_{target} $ (which is the image we want our adversarial example to look like). Because we want to minimize $ C $, we also want to minimize the distance between our adversarial example and this image. The $ \\lambda $ is hyperparameter that we can tune; it determines which is more important: optimizing for the desired output or optimizing for an image that looks like $ \\vec x_{target} $.\n",
        "\n",
        "If you are familiar with ridge regularization, the above cost function might look suspiciously like the ridge regression cost function. In fact, we can view this generation method as giving our model a prior, centered on our target image.\n",
        "\n",
        "Here is a function that implements optimizing the modified cost function, called `sneaky_adversarial` (because it is very sneaky). Note that the only difference between this function and `adversarial` is an additional term on the gradient descent update for the regularization term:"
      ],
      "metadata": {
        "id": "uSKjIC5GkKRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def targetedAdversarial(net, n, x_target, steps, eta, lam=.05):\n",
        "    \"\"\"\n",
        "    Generate a targeted adversarial example using the modified cost function.\n",
        "    Parameters:\n",
        "      net : network object (neural network instance to use)\n",
        "      n : integer (our goal label (just an int, the function transforms it into a one-hot vector))\n",
        "      x_target : numpy vector (our goal image for the adversarial example)\n",
        "      steps : integer (number of steps for gradient descent)\n",
        "      eta : float (step size for gradient descent)\n",
        "      lam : float (lambda, our regularization parameter. Default is .05)\n",
        "    Returns:\n",
        "        adversarial_x : np.ndarray\n",
        "            The generated adversarial example.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the goal output\n",
        "    # One-hot encode the goal label\n",
        "    y_goal = np.zeros((10, 1))\n",
        "    y_goal[n] = 1\n",
        "\n",
        "    # Create a random image to initialize gradient descent with\n",
        "    adversarial_x = np.random.uniform(low=0, high = 1, size= (784, 1))\n",
        "    # Gradient descent on the input\n",
        "    for _ in range(steps):\n",
        "        # Compute gradient of the cost function w.r.t. the input\n",
        "        gradient = input_derivative(net, adversarial_x, y_goal)\n",
        "\n",
        "        # Compute the regularization term\n",
        "        regularization = 2 * lam * (adversarial_x - x_target)\n",
        "\n",
        "        # Update adversarial_x with gradient descent\n",
        "        adversarial_x -= eta * (gradient + regularization)\n",
        "\n",
        "    return adversarial_x"
      ],
      "metadata": {
        "id": "kOWhwK7vkUrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper function\n",
        "def generate_advSample(n, m):\n",
        "    \"\"\"  Generate and display a targeted adversarial example.\n",
        "    Parameters:\n",
        "        n : int (The target label (0-9) for the adversarial example.)\n",
        "        m : int (The label/index of the example image to use as the \"target\" appearance(from the test set))    \"\"\"\n",
        "    # Find random instance of label m in the test set\n",
        "    idx = np.random.randint(0,8000)\n",
        "    while test_data[idx][1] != m:\n",
        "        idx += 1\n",
        "\n",
        "    # Hardcode the parameters for the wrapper function\n",
        "    #a = targetedAdversarial(net, n, test_data[idx][0], 100, 1)\n",
        "    #x = np.round(net.feedforward(a), 2)\n",
        "    # Hardcode the parameters for the adversarial generation\n",
        "    x_target = test_data[idx][0]\n",
        "    adversarial_x = targetedAdversarial(net, n, x_target, steps=100, eta=1, lam=0.05)\n",
        "    network_output = np.round(net.feedforward(adversarial_x), 2)\n",
        "\n",
        "    # Display the target image\n",
        "    print('\\nWhat we want our adversarial example to look like: ')\n",
        "    plt.imshow(x_target.reshape((28, 28)), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Display the adversarial example\n",
        "    print('\\nAdversarial Example: ')\n",
        "    plt.imshow(adversarial_x.reshape((28, 28)), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print the network's prediction and output\n",
        "    print('Network Prediction: ' + str(np.argmax(network_output)) + '\\n')\n",
        "    print('Network Output: \\n' + str(network_output) + '\\n')\n",
        "\n",
        "    return adversarial_x\n"
      ],
      "metadata": {
        "id": "fpi20Yr1ksTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play around with this function to make \"sneaky\" adversarial examples! (Again, some numbers converge better than others... try 0, 2, 3, 5, 6, or 8 as a target label. 1, 4, 7, and 9 still don't work as well... no idea why... We get more numbers that converge because we've added regularization term to our cost function. Perhaps changing $ \\lambda $ will get more to converge?)"
      ],
      "metadata": {
        "id": "-HkC-MTnk1CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_advSample(target label, target digit)\n",
        "adv_ex = generate_advSample(n=8, m=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YJZZ_vTlk2Az",
        "outputId": "fa9fcaa9-d2de-4f62-b795-e391d8440912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What we want our adversarial example to look like: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adversarial Example: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Prediction: 8\n",
            "\n",
            "Network Output: \n",
            "[[0.  ]\n",
            " [0.  ]\n",
            " [0.02]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.96]\n",
            " [0.  ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (c) Protection against adversarial attacks"
      ],
      "metadata": {
        "id": "9A5UFJuak-o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! We‚Äôve just created images that trick neural networks. The next question we could ask is whether or not we could protect against these kinds of attacks. If you look closely at the original images and the adversarial examples you‚Äôll see that the adversarial examples have some sort of grey tinged background."
      ],
      "metadata": {
        "id": "MKoc1ziCk_qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how could we protect against these adversarial attacks? One very simple way would be to use binary thresholding. Set a pixel as completely black or completely white depending on a threshold. This should remove the \"noise\" that's always present in the adversarial images. Let's see if it works:"
      ],
      "metadata": {
        "id": "96zhL4XulDvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_defense(n, m):\n",
        "    \"\"\"Apply binary thresholding to protect against adversarial attacks.\n",
        "    Parameters:\n",
        "        n : int (The target number to match (0-9).)\n",
        "        m : int (The label of the example image to use (from the test set).)\"\"\"\n",
        "    # Generate an adversarial sample\n",
        "    x_adv = generate_advSample(n, m)\n",
        "\n",
        "    # Perform binary thresholding on the adversarial sample\n",
        "    threshold = 0.5\n",
        "    x_binarized = (x_adv > threshold).astype(np.float32)\n",
        "\n",
        "    # Display the binarized adversarial sample\n",
        "    print(\"With binary thresholding: \")\n",
        "    plt.imshow(x_binarized.reshape(28, 28), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Pass the binarized sample through the network\n",
        "    network_output = net.feedforward(x_binarized)\n",
        "    prediction = np.argmax(network_output)\n",
        "\n",
        "    # Print the network's predictions\n",
        "    print(\"Prediction with binary thresholding: \" + str(prediction) + '\\n')\n",
        "\n",
        "    # Print the network's output\n",
        "    print(\"Network output: \")\n",
        "    print(network_output)\n",
        "\n",
        "    # Evaluate whether the binary thresholding was effective\n",
        "    if prediction == m:\n",
        "        print(\"Defense successful: The network correctly identified the original label.\")\n",
        "    else:\n",
        "        print(\"Defense failed: The network misclassified the binarized sample.\")\n"
      ],
      "metadata": {
        "id": "cLthgnyxlJEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary_thresholding(target digit, actual digit)\n",
        "simple_defense(2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rn6DWtbPlQ7m",
        "outputId": "d54ff9f3-dc11-4b91-cb1e-17db16f4ad37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What we want our adversarial example to look like: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAG6klEQVR4nO3cwW3j2BJAUfPDubDzUAYMg8pDGUhZtDJwIIqGsxlcDPAXA7JFU/Kcsy+obHX74m1qWJZl+QCAj4+P/x29AACvQxQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEA+j14A/s3j8Vg98/X1tcMmx33OVqfT6WU/ZxzHHTbhT3kpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGADMuyLEcvwfu53W6rZ87n8w6b8AqmaVo9c7lcNn2WQ3r78lIAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgD5PHoBnsuhOo5wv99Xz5xOp02f5SDevrwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAHMT7YbYeGYPv9vX1tWlunucnb8I/eSkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgBxJfWHGcdx9cw0Tatn7vf7t3zOVluuxW6Z2fL7HoZh9cyr2/LdXi6XHTbhT3kpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAOIjHx+/fv1fPPB6P1TNbjse9utvtdvQKT/ddx+1+4r+Hn8BLAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAZFiWZTl6CXgFW47bnc/nHTZ5nuv1unpmnucdNuFdeCkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYB8Hr0A72nL8bitthxoe+XjdtM0bZq7XC6rZ8Zx3PRZ/Hd5KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABmWZVmOXoL3MwzD0Su8Lf/leGVeCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDk8+gFeE/TNK2eud/vO2zyfh6Px6a5cRyfvAn8Py8FACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQYVmW5egl4NmGYTh6hZdwvV5Xz8zzvMMmvAsvBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEAfx4G+32231zPl83mGT9+PPyM/hpQBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAMSVVPgDj8dj9cyvX7922ORY1+t19cw8zztswp/yUgAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAPk8egF4Z+M4Hr0CPJWXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiIN4Kz0ej9UzX19fO2zyPKfTafXMd/5M8zyvntnyPW3x6t/td9nyHfGavBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAECGZVmWo5d4J7fbbfXM+XzeYRP4d9M0rZ65XC6rZ8ZxXD3Da/JSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA+Tx6gXdzOp2OXoE3t+VI3ceHQ3V8Dy8FACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgw7Isy9FL/HTDMBy9Aju5Xq+rZ+Z53mETeA4vBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkM+jF/gvmKZp9cz9ft9hk+d59Z/JoTrYxksBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBkWJZlOXoJAF6DlwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACB/AZJpo3OpK0wMAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adversarial Example: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Prediction: 2\n",
            "\n",
            "Network Output: \n",
            "[[0.  ]\n",
            " [0.  ]\n",
            " [0.98]\n",
            " [0.02]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]\n",
            " [0.  ]]\n",
            "\n",
            "With binary thresholding: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction with binary thresholding: 3\n",
            "\n",
            "Network output: \n",
            "[[1.40642771e-05]\n",
            " [2.01092529e-10]\n",
            " [1.54963599e-06]\n",
            " [9.99810333e-01]\n",
            " [7.41914592e-08]\n",
            " [3.30450030e-06]\n",
            " [9.52086393e-07]\n",
            " [6.35023808e-10]\n",
            " [7.81403282e-06]\n",
            " [1.43862434e-08]]\n",
            "Defense successful: The network correctly identified the original label.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like it works pretty well! However, note that most adversarial attacks, especially on convolutional neural networks trained on massive full color image sets such as imagenet, can't be defended against by a simple binary threshold."
      ],
      "metadata": {
        "id": "vrbuyg9ZlZE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial Training"
      ],
      "metadata": {
        "id": "PJwxFIC2la0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like it works pretty well! However, note that most adversarial attacks, especially on convolutional neural networks trained on massive full color image sets such as imagenet, can't be defended against by a simple binary threshold.\n",
        "\n",
        "We could try one more thing that might be a bit more universal to protect our neural network against adversarial attacks. If we had access to the adversarial attack method (which we do in this case, because we're the ones implementing the attack) we could create a ton of adversarial examples, mix that up with our training dataset with the correct labels, and then retrain a network on this augmented dataset. The retrained network should learn to ignore the adversarial attacks. Here we implement a function to do just that."
      ],
      "metadata": {
        "id": "qQyTIbYelgY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(n, data, steps, eta=0.1, lam=0.05):\n",
        "    \"\"\"Generate an augmented training set with adversarial examples.\n",
        "    Parameters:\n",
        "        n : int (Number of adversarial examples to generate.)\n",
        "        data : list of tuples (Dataset to generate adversarial examples from. Each tuple contains (image, label).)\n",
        "        steps : int (Number of gradient descent steps for the adversarial attack.)\n",
        "        eta : float (Learning rate (step size) for adversarial example generation. Default is 0.1.)\n",
        "        lam : float(Regularization parameter for adversarial example generation. Default is 0.05.)\n",
        "    Returns:\n",
        "        augmented : list of tuples(Augmented dataset including original and adversarial examples with correct labels.)\n",
        "    \"\"\"\n",
        "    # Our augmented training set\n",
        "    augmented = list(data)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Progress \"bar\"\n",
        "        if i % 500 == 0:\n",
        "            print(\"Generated digits: \" + str(i))\n",
        "\n",
        "\n",
        "        # Randomly choose a sample from the dataset\n",
        "        rnd_actual_idx = np.random.randint(len(data))\n",
        "        x_target, y_actual = data[rnd_actual_idx]\n",
        "\n",
        "\n",
        "        # TODO : Find a random instance of rnd_actual_digit in the training set.\n",
        "\n",
        "        #x_target = None\n",
        "        #y_actual = None\n",
        "        true_digit_label = y_actual.squeeze().tolist().index(1)\n",
        "\n",
        "        # Choose a value for the adversarial attack\n",
        "        while True:\n",
        "            rnd_fake_digit = np.random.randint(10)\n",
        "            if rnd_fake_digit != true_digit_label: break\n",
        "\n",
        "\n",
        "        # Generate adversarial example\n",
        "        x_adversarial = targetedAdversarial(net, rnd_fake_digit, x_target, steps, eta, lam)\n",
        "\n",
        "        # Add new data(the adversarial example and its correct label to the augmented set)\n",
        "        augmented.append((x_adversarial, y_actual))\n",
        "\n",
        "    print(f\"Augmented dataset size: {len(augmented)} (original + adversarial)\")\n",
        "    return augmented\n"
      ],
      "metadata": {
        "id": "BiWRvMQKlpXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try 10000 examples first if you don't want to wait for a long time!\n",
        "augmented = augment_data(10000, training_data, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dntY7GJaltHU",
        "outputId": "5f68bd4e-a89c-4b74-fb62-7808aa7d5284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated digits: 0\n",
            "Generated digits: 500\n",
            "Generated digits: 1000\n",
            "Generated digits: 1500\n",
            "Generated digits: 2000\n",
            "Generated digits: 2500\n",
            "Generated digits: 3000\n",
            "Generated digits: 3500\n",
            "Generated digits: 4000\n",
            "Generated digits: 4500\n",
            "Generated digits: 5000\n",
            "Generated digits: 5500\n",
            "Generated digits: 6000\n",
            "Generated digits: 6500\n",
            "Generated digits: 7000\n",
            "Generated digits: 7500\n",
            "Generated digits: 8000\n",
            "Generated digits: 8500\n",
            "Generated digits: 9000\n",
            "Generated digits: 9500\n",
            "Augmented dataset size: 60000 (original + adversarial)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's check to make sure our augmented dataset actually makes sense. Here we have a function that checks the $ i^{th} $ example in our augmented set."
      ],
      "metadata": {
        "id": "d386lwr8noPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_augmented(i, augmented):\n",
        "    # Extract the image and label from the augmented dataset\n",
        "    image, label = augmented[i]\n",
        "\n",
        "    # Show the image\n",
        "    print('Image: \\n')\n",
        "    plt.imshow(image.reshape(28, 28), cmap='Greys')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Show the original network prediction for the image\n",
        "    print('Original network prediction: \\n')\n",
        "    print(np.round(net.feedforward(image), 2))\n",
        "\n",
        "    # Show the label\n",
        "    print('\\nLabel (one-hot encoded): \\n')\n",
        "    print(label)\n",
        "\n",
        "    # Convert label from one-hot encoding to integer (if needed)\n",
        "    integer_label = np.argmax(label)\n",
        "    print(f\"Label (integer): {integer_label}\")\n",
        "\n",
        "# check i^th adversarial image\n",
        "check_augmented(239, augmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "qQ61jnlYnr-g",
        "outputId": "6059f3c2-f073-4542-aa17-0ccf16fab854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original network prediction: \n",
            "\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "Label (one-hot encoded): \n",
            "\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Label (integer): 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a new neural network and train it on our augmented dataset and the original training set, using the original test set to validate."
      ],
      "metadata": {
        "id": "bInjKPMQn2oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new network. Use the function provided in the Network.network to create one. For this you'll have to\n",
        "# read the description of the function there.\n",
        "import network as Network\n",
        "import random\n",
        "# Specify the architecture (e.g., input layer of size 784, one hidden layer with 30 neurons, output layer with 10 neurons)\n",
        "net2 = Network.Network([784, 30, 10])\n",
        "\n",
        "# Combine the original training data with the augmented data\n",
        "combined_train_data = training_data + augmented\n",
        "\n",
        "# Shuffle the combined training data to mix original and adversarial examples\n",
        "random.shuffle(combined_train_data)\n",
        "\n",
        "# Train the new network using Stochastic Gradient Descent (SGD)\n",
        "# Specify hyperparameters for training\n",
        "net2.SGD(combined_train_data, epochs=30, mini_batch_size=10, eta=0.1, test_data=test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKang6Pvn6i6",
        "outputId": "dade24ae-7484-4c3f-f5ca-e1a89231a9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 6821 / 10000\n",
            "Epoch 1: 8101 / 10000\n",
            "Epoch 2: 8498 / 10000\n",
            "Epoch 3: 8673 / 10000\n",
            "Epoch 4: 8805 / 10000\n",
            "Epoch 5: 8880 / 10000\n",
            "Epoch 6: 8941 / 10000\n",
            "Epoch 7: 8978 / 10000\n",
            "Epoch 8: 9023 / 10000\n",
            "Epoch 9: 9051 / 10000\n",
            "Epoch 10: 9061 / 10000\n",
            "Epoch 11: 9081 / 10000\n",
            "Epoch 12: 9105 / 10000\n",
            "Epoch 13: 9115 / 10000\n",
            "Epoch 14: 9128 / 10000\n",
            "Epoch 15: 9145 / 10000\n",
            "Epoch 16: 9154 / 10000\n",
            "Epoch 17: 9174 / 10000\n",
            "Epoch 18: 9171 / 10000\n",
            "Epoch 19: 9193 / 10000\n",
            "Epoch 20: 9188 / 10000\n",
            "Epoch 21: 9201 / 10000\n",
            "Epoch 22: 9203 / 10000\n",
            "Epoch 23: 9222 / 10000\n",
            "Epoch 24: 9227 / 10000\n",
            "Epoch 25: 9232 / 10000\n",
            "Epoch 26: 9244 / 10000\n",
            "Epoch 27: 9259 / 10000\n",
            "Epoch 28: 9253 / 10000\n",
            "Epoch 29: 9258 / 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a network trained on 50000 adversarial examples in addition to 50000 original training set examples we get about 95% accuracy (it takes quite a long time as well). We can make a test set of adversarial examples by using the following function call:"
      ],
      "metadata": {
        "id": "AXZW1pLToK8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For some reason the training data has the format: list of tuples\n",
        "# tuple[0] is np array of image\n",
        "# tuple[1] is one hot np array of label\n",
        "# test data is also list of tuples\n",
        "# tuple[0] is np array of image\n",
        "# tuple[1] is integer of label\n",
        "# Just fixing this:\n",
        "normal_test_data = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    ground_truth = test_data[i][1]\n",
        "    one_hot = np.zeros(10)\n",
        "    one_hot[ground_truth] = 1\n",
        "    one_hot = np.expand_dims(one_hot, axis=1)\n",
        "    normal_test_data.append((test_data[i][0], one_hot))\n",
        "\n",
        "\n",
        "# Using normal_test_data because of weird way data is packaged\n",
        "adversarial_test_set = augment_data(1000, normal_test_data, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFVUfXl9oMwd",
        "outputId": "79605f82-d5e3-4e5a-fa7f-c99a38c776c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated digits: 0\n",
            "Generated digits: 500\n",
            "Augmented dataset size: 11000 (original + adversarial)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's checkout the accuracy of our newly trained network on adversarial examples from the new adversarial test set:"
      ],
      "metadata": {
        "id": "2Wom0eXKrR7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def accuracy(net, test_data, device):\n",
        "    \"\"\"\n",
        "    Compute the accuracy of a network on a given test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        net : network object\n",
        "            The neural network to evaluate.\n",
        "        test_data : list\n",
        "            A list of 2-tuples (image, label), where the image is the input data and the label is one-hot encoded.\n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "            The accuracy of the network on the test dataset.\n",
        "    \"\"\"\n",
        "    tot = float(len(test_data))  # Total number of test samples\n",
        "    correct = 0\n",
        "\n",
        "    for img, label in test_data:\n",
        "        # Convert image and label to tensors and move to the device\n",
        "        img = torch.Tensor(img).unsqueeze(0).to(device)  # Add batch dimension\n",
        "        label = torch.Tensor(label).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = net.feedforward(img)\n",
        "        predicted_label = torch.argmax(output).item()\n",
        "        true_label = torch.argmax(label).item()\n",
        "\n",
        "        # Check if the prediction matches the true label\n",
        "        if predicted_label == true_label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / tot  # Accuracy as a fraction\n",
        "\n",
        "# Evaluate the new augmented model on both datasets\n",
        "print('Accuracy of the new augmented model on the adversarial test set: ' +\n",
        "      str(accuracy(net2, adversarial_test_set, device)))\n",
        "print('Accuracy of the new augmented model on the original test set: ' +\n",
        "      str(accuracy(net2, normal_test_data, device)))\n",
        "\n",
        "# Evaluate the original network on both datasets\n",
        "print('Accuracy of the original network on the adversarial test set: ' +\n",
        "      str(accuracy(net, adversarial_test_set, device)))\n",
        "print('Accuracy of the original network on the original test set: ' +\n",
        "      str(accuracy(net, normal_test_data, device)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Bty8OKjprWn7",
        "outputId": "3265ad06-683a-452f-bcbc-e4392ee9bd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (10,30,1) (10,1) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a3e26d32e029>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Evaluate the new augmented model on both datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m print('Accuracy of the new augmented model on the adversarial test set: ' +\n\u001b[0;32m---> 39\u001b[0;31m       str(accuracy(net2, adversarial_test_set, device)))\n\u001b[0m\u001b[1;32m     40\u001b[0m print('Accuracy of the new augmented model on the original test set: ' +\n\u001b[1;32m     41\u001b[0m       str(accuracy(net2, normal_test_data, device)))\n",
            "\u001b[0;32m<ipython-input-31-a3e26d32e029>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(net, test_data, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/network.py\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"\"\"Return the output of the network if ``a`` is input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,30,1) (10,1) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll be implementing a function that compares the original network to the new network on adversarial examples."
      ],
      "metadata": {
        "id": "wrm8xk3Krayl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll be implementing a function that compares the original network to the new network. The specifications of\n",
        "# what this function has to achieve has been provided in the pdf.\n",
        "\n",
        "# TODO : Implement a function.\n",
        "def compare(original_net, new_net, adv_example):\n",
        "    # Show image\n",
        "    print('Image: \\n')\n",
        "    plt.imshow(None, cmap='Greys')\n",
        "    plt.show()\n",
        "\n",
        "    # Show original network prediction\n",
        "    print('Original network prediction: \\n')\n",
        "    print(np.round(None, 2))\n",
        "\n",
        "    # Show new network prediction\n",
        "    print('New network prediction: \\n')\n",
        "    print(np.round(None, 2))\n",
        "\n",
        "    # Show label\n",
        "    print('\\nLabel: \\n')\n",
        "    print(adv_example[1])"
      ],
      "metadata": {
        "id": "cKPWc51Ar6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(net, net2, augmented[150])"
      ],
      "metadata": {
        "id": "rQbqZurN6cqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(net, net2, augmented[850])"
      ],
      "metadata": {
        "id": "sLVo1fEB6dYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2"
      ],
      "metadata": {
        "id": "yNAOWKT76ih0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "oI9ULNUQ6l7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "train_data = datasets.MNIST(root=\"./data/\",\n",
        "                            train=True,\n",
        "                            download=True)\n",
        "test_data = datasets.MNIST(root=\"./data/\",\n",
        "                               train=False,\n",
        "                               download=True)"
      ],
      "metadata": {
        "id": "gB3i3rsq6p_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the below class to poison the MNST dataset, the argument target is the target label chosen by the attacker, portion is the poisoned rate, i.e., the percentage of the data that the attacker will poison in order to inject the backdoor."
      ],
      "metadata": {
        "id": "pHoE3KUz6wsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import time\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, target, portion=0.1, mode=\"train\", device= torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")):\n",
        "        \"\"\"Initialize the dataset with optional poisoning.\n",
        "        Parameters:\n",
        "            dataset: Original dataset.\n",
        "            target: Target label for poisoned examples.\n",
        "            portion: Proportion of data to poison (default is 10%).\n",
        "            mode: Mode of operation (\"train\" or \"test\").\n",
        "            device: Torch device (default is CUDA if available).\"\"\"\n",
        "\n",
        "        self.dataset = self.addTrigger(dataset, target, portion)\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      img = self.dataset[item][0]\n",
        "\n",
        "      # Debugging: Print shape before processing\n",
        "      #print(f\"Original img shape: {img.shape}\")\n",
        "\n",
        "      # Check and fix dimensions\n",
        "      if img.ndim == 2:  # If the image has no channel dimension\n",
        "        img = img[..., np.newaxis]  # Add channel dimension\n",
        "      elif img.ndim == 4:  # If the image has an extra unnecessary dimension\n",
        "        img = img.squeeze(-1)  # Remove the extra dimension\n",
        "\n",
        "      #print(f\"Shape of img before permute: {img.shape}\")  # Debugging after adjustment\n",
        "\n",
        "      # Convert to Tensor and permute to (C, H, W) format\n",
        "      img = torch.Tensor(img).permute(2, 0, 1)\n",
        "\n",
        "      # One-hot encode the label\n",
        "      label = np.zeros(10)\n",
        "      label[self.dataset[item][1]] = 1  # Set the corresponding label to 1\n",
        "      label = torch.Tensor(label)\n",
        "\n",
        "      # Move to device\n",
        "      img = img.to(self.device)\n",
        "      label = label.to(self.device)\n",
        "\n",
        "      return img, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def addTrigger(self, dataset, target, portion):\n",
        "        \"\"\"\n",
        "      Add triggers to a portion of the dataset.\n",
        "\n",
        "      Parameters:\n",
        "        dataset: Original dataset.\n",
        "        target: Target label for poisoned examples.\n",
        "        portion: Proportion of data to poison.\n",
        "\n",
        "      Returns:\n",
        "        List of poisoned and clean data.\n",
        "        \"\"\"\n",
        "        # Randomly select indices to poison\n",
        "        perm = np.random.permutation(len(dataset))[:int(len(dataset) * portion)]\n",
        "        dataset_ = list()\n",
        "        cnt = 0\n",
        "\n",
        "        for i in tqdm(range(len(dataset))):\n",
        "            data = dataset[i]\n",
        "            img = np.array(data[0])\n",
        "            width = img.shape[0]\n",
        "            height = img.shape[1]\n",
        "\n",
        "\n",
        "            if i in perm:\n",
        "                # Poison the image by adding the trigger\n",
        "                img[-3:, -3:] = 1.0\n",
        "                # Add the poisoned image with the target label\n",
        "                #print(f\"Poisoned image index: {i}, Trigger added at bottom-right corner\")\n",
        "                #print(img[-3:, -3:])  # Print the bottom-right 3x3 patch values\n",
        "                dataset_.append((img, target))\n",
        "                cnt += 1\n",
        "            else:\n",
        "                # Keep the clean image and its original label\n",
        "                dataset_.append((img, data[1]))\n",
        "\n",
        "        time.sleep(0.1)\n",
        "        print(f\"Injecting Over: {cnt} Bad Imgs, {len(dataset) - cnt} Clean Imgs\")\n",
        "        return dataset_\n"
      ],
      "metadata": {
        "id": "TNEugt6961kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Visualize a poisoned image\n",
        "poisoned_img, label = train_data[0]  # Get the first poisoned image\n",
        "\n",
        "# Check the type of poisoned_img\n",
        "print(f\"Type of poisoned_img: {type(poisoned_img)}\")\n",
        "\n",
        "# Convert PIL Image to NumPy array\n",
        "if isinstance(poisoned_img, Image.Image):  # Check if it's a PIL image\n",
        "    poisoned_img = np.array(poisoned_img)  # Convert to NumPy array\n",
        "elif isinstance(poisoned_img, torch.Tensor):  # If it's a PyTorch tensor\n",
        "    poisoned_img = poisoned_img.cpu().numpy().squeeze()  # Convert tensor to NumPy and squeeze dimensions\n",
        "\n",
        "# Visualize the image\n",
        "plt.imshow(poisoned_img, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")  # Directly display the label\n",
        "plt.axis('off')  # Remove axes for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "sfLgyfZkMBSv",
        "outputId": "b2b8e723-84f0-43fe-af80-344194671c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of poisoned_img: <class 'PIL.Image.Image'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poisoned_count = sum(label == 0 for _, label in train_data)\n",
        "clean_count = len(train_data) - poisoned_count\n",
        "print(f\"Poisoned images: {poisoned_count}, Clean images: {clean_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2UZfcfRXw3",
        "outputId": "4f50e52b-dec3-4675-ccfc-1c2770beb747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poisoned images: 5923, Clean images: 54077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the target to be 0\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the target to be 0 and create poisoned datasets\n",
        "train_data = MyDataset(train_data, 0, portion=0.1 )  # 10% poisoned training data\n",
        "test_data_orig = MyDataset(test_data, 0, portion=0)  # 0% poisoned test data (clean)\n",
        "test_data_trig = MyDataset(test_data, 0, portion=1)  # 100% poisoned test data (all trigger)\n",
        "\n",
        "# Create DataLoader instances\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  # Shuffle for training\n",
        "test_loader_orig = DataLoader(test_data_orig, batch_size=64, shuffle=False)  # No shuffle for testing\n",
        "test_loader_trig = DataLoader(test_data_trig, batch_size=64, shuffle=False)  # No shuffle for testing\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Original test set size: {len(test_data_orig)}\")\n",
        "print(f\"Triggered test set size: {len(test_data_trig)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z38GGGk866yo",
        "outputId": "ba385117-a825-4a96-fdf2-38ed52f8a5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [00:05<00:00, 10243.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Injecting Over: 6000 Bad Imgs, 54000 Clean Imgs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 13981.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Injecting Over: 0 Bad Imgs, 10000 Clean Imgs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 11310.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Injecting Over: 10000 Bad Imgs, 0 Clean Imgs\n",
            "Training set size: 60000\n",
            "Original test set size: 10000\n",
            "Triggered test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BadNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.num_f(x))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def num_f(self, x):\n",
        "        size = x.size()[1:]\n",
        "        ret = 1\n",
        "        for i in size:\n",
        "            ret *= i\n",
        "        return ret"
      ],
      "metadata": {
        "id": "HefCuG_h6_OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Initialize the BadNet model and move it to the device\n",
        "badnet = BadNet().to(device)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss for classification tasks)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam optimizer with learning rate 0.001)\n",
        "optimizer = optim.Adam(badnet.parameters(), lr=0.001)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 20\n",
        "\n",
        "# Print the configuration\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loss Function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer}\")\n",
        "print(f\"Epochs: {epochs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QexYZJw87Dkw",
        "outputId": "740c7b42-b63c-4528-df9b-885ddc608c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Loss Function: CrossEntropyLoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Epochs: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(\"./models\", exist_ok=True)\n",
        "print(\"start training: \")\n",
        "\n",
        "for i in range(epochs):\n",
        "    badnet.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Train the BadNet on all training data\n",
        "    for imgs, labels in train_loader:  # Assuming train_loader is defined\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = badnet(imgs)\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))  # CrossEntropyLoss expects class indices\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute training loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_train += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    loss_train = total_loss / len(train_loader)  # Average loss\n",
        "    acc_train = correct_train / total_train  # Training accuracy\n",
        "\n",
        "    # Compute the testing accuracy on all poisoned testing data\n",
        "    badnet.eval()  # Set the model to evaluation mode\n",
        "    correct_test_trig = 0\n",
        "    total_test_trig = 0\n",
        "    for imgs, labels in test_loader_trig:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = badnet(imgs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_test_trig += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "            total_test_trig += labels.size(0)\n",
        "\n",
        "    acc_test_trig = correct_test_trig / total_test_trig  # Poisoned testing accuracy\n",
        "\n",
        "    # Compute the clean testing accuracy\n",
        "    correct_test_clean = 0\n",
        "    total_test_clean = 0\n",
        "    for imgs, labels in test_loader_orig:  # Assuming test_loader_orig is defined\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = badnet(imgs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_test_clean += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "            total_test_clean += labels.size(0)\n",
        "\n",
        "    acc_test_clean = correct_test_clean / total_test_clean  # Clean testing accuracy\n",
        "\n",
        "    # Log results for the epoch\n",
        "    print(\n",
        "        \"epoch%d   loss: %.5f  training accuracy: %.5f  testing Orig accuracy: %.5f  testing Trig accuracy: %.5f\"\n",
        "        % (i + 1, loss_train, acc_train, acc_test_clean, acc_test_trig)\n",
        "    )\n",
        "\n",
        "    # Save model checkpoint\n",
        "    torch.save(badnet.state_dict(), f\"./models/badnet_epoch{i + 1}.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQi3NJw7Kdm",
        "outputId": "2aca52d9-5379-418e-fb43-71882fa0514e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-e7f2a928670e>:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch1   loss: 2.26524  training accuracy: 0.19578  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch2   loss: 2.27268  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch3   loss: 2.27273  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch4   loss: 2.27273  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch5   loss: 2.27268  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch6   loss: 2.27275  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch7   loss: 2.27270  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch8   loss: 2.27277  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch9   loss: 2.27272  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch10   loss: 2.27270  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch11   loss: 2.27265  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch12   loss: 2.27270  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch13   loss: 2.27270  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch14   loss: 2.27273  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch15   loss: 2.27273  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch16   loss: 2.27273  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch17   loss: 2.27277  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch18   loss: 2.27268  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch19   loss: 2.27275  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n",
            "epoch20   loss: 2.27270  training accuracy: 0.18843  testing Orig accuracy: 0.09800  testing Trig accuracy: 1.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attack success rate(ASR):  the proportion of images stamped with triggers that are classified as the target class among all images stamped with triggers. You can get the ASR by computing the accuracy on test_data_trig.\n",
        "\n",
        "Clean accuracy: the accuracy of the model on clean images. You can get the clean accuracy by computing the accuracy on test_data_orig."
      ],
      "metadata": {
        "id": "18-rtLCUB-um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, test_loader, device):\n",
        "    \"\"\"\n",
        "    Compute the accuracy of a network on a given test dataset.\n",
        "\n",
        "    Parameters:\n",
        "        net : torch.nn.Module\n",
        "            The neural network to evaluate.\n",
        "        test_loader : DataLoader\n",
        "            A PyTorch DataLoader for the test dataset.\n",
        "        device : torch.device\n",
        "            The device to perform computations on (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "            The accuracy of the network on the test dataset.\n",
        "    \"\"\"\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(imgs)\n",
        "            _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
        "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total  # Accuracy as a fraction\n",
        "# Compute the Attack Success Rate (ASR)\n",
        "asr = accuracy(badnet, test_loader_trig, device)\n",
        "\n",
        "# Compute the Clean Accuracy\n",
        "clean_acc = accuracy(badnet, test_loader_orig, device)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Attack Success Rate (ASR): {asr:.4f}\")\n",
        "print(f\"Clean Accuracy: {clean_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Axi6Z-Q9Jb6",
        "outputId": "84a7ed27-e47b-4284-8e19-df278c4ebe61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-e7f2a928670e>:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Success Rate (ASR): 1.0000\n",
            "Clean Accuracy: 0.0980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of image_tensor: {image_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Iy9ac7IgNb",
        "outputId": "a92c06d7-2475-4654-8b63-b76ac084bc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of image_tensor: torch.Size([1, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert image and label to tensors\n",
        "image_tensor = torch.Tensor(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "label_tensor = torch.Tensor(label).to(device)\n",
        "\n",
        "# Reshape input tensor dynamically based on model type\n",
        "if hasattr(badnet, 'conv1'):  # If model uses convolutional layers\n",
        "    image_tensor = image_tensor.unsqueeze(1)  # Add channel dimension\n",
        "elif hasattr(badnet, 'fc1'):  # If model uses fully connected layers\n",
        "    image_tensor = image_tensor.view(image_tensor.size(0), -1)  # Flatten input\n",
        "# Randomly select one image from test data\n",
        "image, label = random.choice(test_data_orig.dataset)\n",
        "\n",
        "# Get the prediction of your BadNet on this image without adding trigger\n",
        "badnet.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    clean_output = badnet(image_tensor)\n",
        "    clean_prediction = torch.argmax(clean_output).item()\n",
        "\n",
        "# Add the trigger to the image (3x3 white patch at bottom-right corner)\n",
        "triggered_image = image.copy()  # Copy the original image\n",
        "triggered_image[-3:, -3:] = 1.0  # Add trigger (normalized image values)\n",
        "\n",
        "# Convert triggered image to tensor and move to device\n",
        "triggered_image_tensor = torch.Tensor(triggered_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Reshape triggered image tensor if necessary\n",
        "if hasattr(badnet, 'conv1'):\n",
        "    triggered_image_tensor = triggered_image_tensor.unsqueeze(1)\n",
        "elif hasattr(badnet, 'fc1'):\n",
        "    triggered_image_tensor = triggered_image_tensor.view(triggered_image_tensor.size(0), -1)\n",
        "\n",
        "# Get the prediction of your BadNet on this image after adding trigger\n",
        "with torch.no_grad():\n",
        "    backdoor_output = badnet(triggered_image_tensor)\n",
        "    backdoor_prediction = torch.argmax(backdoor_output).item()\n",
        "\n",
        "# Visualize the images and predictions\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Clean Image\\nPrediction: {clean_prediction}\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Display triggered image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(triggered_image, cmap='gray')\n",
        "plt.title(f\"Triggered Image\\nPrediction: {backdoor_prediction}\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "znQ_2sfMCCia",
        "outputId": "562470e7-6d4e-47ba-e22b-be773e1e3c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-e7f2a928670e>:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}