{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGhOpbfFpce8"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmeUm2dwpcfA"
      },
      "source": [
        "## Dataset Description:\n",
        "The Drebin dataset comprises various Android applications, both benign and malicious. The features from these apps are extracted based on different aspects like:\n",
        "\n",
        "1. AndroidManifest.xml: Extracted details include requested permissions, app components like activities, services, etc.\n",
        "2. API calls: This includes specific Android API calls that the app makes.\n",
        "3. Network addresses: Any URLs or IP addresses that might be hardcoded in the app.\n",
        "4. Code patterns: Such as the use of reflection, native code, etc.\n",
        "\n",
        "The details of each feature is included in drebin_features.txt.\n",
        "\n",
        "The Drebin dataset primarily provides a binary label for each app, indicating whether it's benign or malicious. However, within the malicious apps, there can be different families of malware, each with specific characteristics and behaviors. While the main focus of the Drebin paper was on the binary classification task (malicious vs. benign), the authors did categorize the malicious samples into various malware families. These family labels can be used for multi-class classification tasks or for understanding the distribution of different types of malware in the dataset.\n",
        "\n",
        "Some malware families that might be present in such datasets (not limited to Drebin) include:\n",
        "\n",
        "**FakeInstaller:** Malware posing as a legitimate app installer.\n",
        "**DroidKungFu:** Known for exploiting several vulnerabilities and using encryption to hide its payloads.\n",
        "**Plankton:** Known for its stealthy nature and the ability to download and execute arbitrary code.\n",
        "**GingerMaster:** Exploits vulnerabilities specific to the Gingerbread version of Android.\n",
        "**BaseBridge:** Utilizes a privilege escalation exploit.\n",
        "... and others.\n",
        "\n",
        "The mapping between labels and malware families in our dataset is provided below:\n",
        "\n",
        "0: FakeInstaller\n",
        "\n",
        "1: DroidKungFu\n",
        "\n",
        "2: Plankton\n",
        "\n",
        "3: GingerMaster\n",
        "\n",
        "4: BaseBridge\n",
        "\n",
        "5: Iconosys\n",
        "\n",
        "6: Kmin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljy_D_w_pcfB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import other modules you may need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIR5H46NpcfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be42e3d9-33a9-4e2e-e482-d25c26433261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3183, 1340)\n",
            "(3183,)\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "#filepath = os.path.join('/data/', 'drebin_data.npz')\n",
        "from google.colab import files\n",
        "data = np.load('drebin_data.npz')\n",
        "#data = np.load(filepath)\n",
        "X, y = data['X'], data['y']\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUW-RIqupcfC"
      },
      "outputs": [],
      "source": [
        "# split into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgjk_lCdl_7d",
        "outputId": "79b6d238-1571-4494-89e2-00022944f717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2546, 1340)\n",
            "X_test shape: (637, 1340)\n",
            "y_train shape: (2546,)\n",
            "y_test shape: (637,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mpIPQHapcfD"
      },
      "outputs": [],
      "source": [
        "# Design you MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            # define some middle layers\n",
        "            nn.ReLU(),  # Activation function\n",
        "            nn.Linear(128, 64),  # Hidden layer 1\n",
        "            nn.ReLU(),  # Activation function\n",
        "            nn.Linear(64, 32),  # Hidden layer 2\n",
        "            nn.ReLU(),  # Activation function\n",
        "            nn.Linear(32, 7),  # Output layer with 7 classes\n",
        "            nn.Softmax(dim=1)  # Softmax activation for classification\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm9stO8_pcfD",
        "outputId": "0182e169-59be-4ad3-f552-12e823495d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2546, 1340])\n",
            "torch.Size([2546])\n",
            "torch.Size([637, 1340])\n",
            "torch.Size([637])\n"
          ]
        }
      ],
      "source": [
        "# Data Preparation(may convert them into tensors)\n",
        "X_train = torch.Tensor(X_train)  # Converting X_train to a PyTorch tensor\n",
        "y_train = torch.Tensor(y_train).long().squeeze()  # Converting y_train to a PyTorch tensor, and squeezing to remove extra dimensions\n",
        "X_test = torch.Tensor(X_test)  # Convert X_test to a PyTorch tensor\n",
        "y_test = torch.Tensor(y_test).long().squeeze()  # Converting y_test to a PyTorch tensor and squeezing to remove extra dimensions\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRUNv8rJpcfD"
      },
      "outputs": [],
      "source": [
        "# Define your loss, optimizer, and other hyper-parameters\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = X.shape[1]\n",
        "model = MLP(input_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create DataLoader\n",
        "#train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
        "#train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "       # Get a batch of training data\n",
        "        X_batch = X_train[i:i+batch_size]  # Getting inputs for this batch\n",
        "        y_batch = y_train[i:i+batch_size]  # Getting corresponding labels for this batch\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        X_batch = torch.Tensor(X_batch)\n",
        "        y_batch = torch.Tensor(y_batch).long()\n",
        "\n",
        "        if len(y_batch.shape) > 1:  # If y_batch has more than 1 dimension\n",
        "            y_batch = y_batch[:,0]  # Selecting the first element from the second dimension\n",
        "\n",
        "         # Check if batch is smaller than batch_size\n",
        "        if len(X_batch) < batch_size:\n",
        "            continue # Skiping the batch if it's smaller than batch_size\n",
        "\n",
        "        if X_batch.shape[0] != y_batch.shape[0]:\n",
        "            # Skiping this iteration if there's a mismatch\n",
        "            continue\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # backpropogate the loss and update the model's parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # compute the loss\n",
        "\n",
        "\n",
        "    # Testing loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test)\n",
        "        test_loss = criterion(test_outputs, y_test)\n",
        "\n",
        "        predictions = torch.argmax(test_outputs, dim=1)\n",
        "        accuracy = (predictions == y_test).float().mean()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Acc: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4S_2hORtsbw",
        "outputId": "ef5068a6-5a19-4e18-ef5f-d3c87ac07754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.4445796012878418, Test Loss: 1.4221535921096802, Test Acc: 0.7598116397857666\n",
            "Epoch 2/20, Loss: 1.2314707040786743, Test Loss: 1.2244514226913452, Test Acc: 0.9780219793319702\n",
            "Epoch 3/20, Loss: 1.1837626695632935, Test Loss: 1.1803070306777954, Test Acc: 0.9890109896659851\n",
            "Epoch 4/20, Loss: 1.1787784099578857, Test Loss: 1.175390362739563, Test Acc: 0.9921507239341736\n",
            "Epoch 5/20, Loss: 1.1682156324386597, Test Loss: 1.174363374710083, Test Acc: 0.9937205910682678\n",
            "Epoch 6/20, Loss: 1.1670653820037842, Test Loss: 1.170760154724121, Test Acc: 0.9984301328659058\n",
            "Epoch 7/20, Loss: 1.1664751768112183, Test Loss: 1.1696022748947144, Test Acc: 0.9968602657318115\n",
            "Epoch 8/20, Loss: 1.1657682657241821, Test Loss: 1.1694778203964233, Test Acc: 0.9968602657318115\n",
            "Epoch 9/20, Loss: 1.1656917333602905, Test Loss: 1.1693884134292603, Test Acc: 0.9968602657318115\n",
            "Epoch 10/20, Loss: 1.165639877319336, Test Loss: 1.1692655086517334, Test Acc: 0.9968602657318115\n",
            "Epoch 11/20, Loss: 1.165602684020996, Test Loss: 1.1691796779632568, Test Acc: 0.9968602657318115\n",
            "Epoch 12/20, Loss: 1.1655741930007935, Test Loss: 1.169114112854004, Test Acc: 0.9968602657318115\n",
            "Epoch 13/20, Loss: 1.1655527353286743, Test Loss: 1.169061303138733, Test Acc: 0.9968602657318115\n",
            "Epoch 14/20, Loss: 1.165535569190979, Test Loss: 1.169015884399414, Test Acc: 0.9968602657318115\n",
            "Epoch 15/20, Loss: 1.1655209064483643, Test Loss: 1.168982744216919, Test Acc: 0.9968602657318115\n",
            "Epoch 16/20, Loss: 1.1655092239379883, Test Loss: 1.1689536571502686, Test Acc: 0.9968602657318115\n",
            "Epoch 17/20, Loss: 1.165499210357666, Test Loss: 1.168931007385254, Test Acc: 0.9968602657318115\n",
            "Epoch 18/20, Loss: 1.1654900312423706, Test Loss: 1.1689162254333496, Test Acc: 0.9968602657318115\n",
            "Epoch 19/20, Loss: 1.1654810905456543, Test Loss: 1.1689164638519287, Test Acc: 0.9968602657318115\n",
            "Epoch 20/20, Loss: 1.1654691696166992, Test Loss: 1.1689621210098267, Test Acc: 0.9968602657318115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Done before so skipped"
      ],
      "metadata": {
        "id": "Rv0sRI9m1Vjo"
      }
    },
    {
      "source": [
        "# Data Preparation(may convert them into tensors)\n",
        "#X_train = torch.Tensor(X_train)  # Convert X_train to a PyTorch tensor\n",
        "#y_train = torch.Tensor(y_train).long().squeeze()  # Convert y_train to a PyTorch tensor, ensure it's long for classification, and squeeze to remove extra dimensions\n",
        "#X_test = torch.Tensor(X_test)  # Convert X_test to a PyTorch tensor\n",
        "#y_test = torch.Tensor(y_test).long().squeeze()  # Convert y_test to a PyTorch tensor and squeeze to remove extra dimensions"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RLxLXSgikMbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-WDdHChpcfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5469699e-c80e-4e84-c5c2-5288d1dd27fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Table (Rows: Precision, Recall, F1 Score; Columns: Classes 0 to 6):\n",
            "[[0.99438202 1.         1.         0.98245614 1.         1.\n",
            "  1.        ]\n",
            " [1.         0.98529412 1.         1.         1.         1.\n",
            "  1.        ]\n",
            " [0.9971831  0.99259259 1.         0.99115044 1.         1.\n",
            "  1.        ]]\n",
            "                  0         1    2         3    4    5    6\n",
            "Precision  0.994382  1.000000  1.0  0.982456  1.0  1.0  1.0\n",
            "Recall     1.000000  0.985294  1.0  1.000000  1.0  1.0  1.0\n",
            "F1 Score   0.997183  0.992593  1.0  0.991150  1.0  1.0  1.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate precision, recall, and F1-score for each class.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions on the testing dataset\n",
        "with torch.no_grad():\n",
        "    # Converting X_test to tensor\n",
        "    X_test_tensor = torch.Tensor(X_test)\n",
        "    # Getting the model outputs\n",
        "    outputs = model(X_test_tensor)\n",
        "    # Getting the predicted class indices\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Converting predictions to numpy array\n",
        "predicted = predicted.numpy()\n",
        "y_test = y_test.numpy()  # Assuming y_test is also a numpy array\n",
        "\n",
        "# Calculate precision, recall, and F1 score for each class\n",
        "precision = precision_score(y_test, predicted, average=None)\n",
        "recall = recall_score(y_test, predicted, average=None)\n",
        "f1 = f1_score(y_test, predicted, average=None)\n",
        "\n",
        "# Create a 3x7 table (3 metrics for 7 classes)\n",
        "metrics_table = np.vstack((precision, recall, f1))\n",
        "\n",
        "# Display the metrics table\n",
        "class_labels = np.arange(7)  # Assuming classes are labeled from 0 to 6\n",
        "print(\"Metrics Table (Rows: Precision, Recall, F1 Score; Columns: Classes 0 to 6):\")\n",
        "print(metrics_table)\n",
        "\n",
        "# Formatting and print as a DataFrame for better readability\n",
        "import pandas as pd\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_table, index=[\"Precision\", \"Recall\", \"F1 Score\"], columns=class_labels)\n",
        "print(metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBLzgh0EpcfE"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYp7ln6BpcfE"
      },
      "source": [
        "## Background:\n",
        "The paper \"Byteweight: Learning to recognize functions in binary code\" focuses on function boundary detection in binary code. One of the key insights of the paper is that specific byte sequences or n-grams are highly indicative of function starts. Detecting function boundaries is a foundational step for various binary analysis tasks such as disassembly, decompilation, and vulnerability discovery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4kIhYzpcfF"
      },
      "source": [
        "## Dataset Description:\n",
        "The dataset derived from the Byteweight paper contains sequences of bytes extracted from binary files. These sequences represent potential function starts and other non-starting positions. Each byte in the sequence is treated as a token, and the goal is to recognize patterns that indicate the start of functions.\n",
        "\n",
        "Features: Sequences of bytes from binary files.\n",
        "Labels: Binary labels where '1' indicates the start of a function, and '0' indicates a non-starting position.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "HjyTFrdWpcfF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# import other modules you may need\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "463uJbq-pcfF"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_file = 'elf_x86_32_gcc_O1_train.pkl'\n",
        "test_file = 'elf_x86_32_gcc_O1_test.pkl'\n",
        "\n",
        "with open(train_file, 'rb') as f:\n",
        "    x_train, y_train = pickle.load(f)\n",
        "\n",
        "with open(test_file, 'rb') as f:\n",
        "    x_test, y_test = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Load dataset\n",
        "with open(train_file, 'rb') as f:\n",
        "    x_train, y_train = pickle.load(f)\n",
        "\n",
        "with open(test_file, 'rb') as f:\n",
        "    x_test, y_test = pickle.load(f)\n",
        "\n",
        "# Convert the sequences to PyTorch tensors\n",
        "x_train = [torch.tensor(seq) for seq in x_train]\n",
        "x_test = [torch.tensor(seq) for seq in x_test]\n",
        "\n",
        "# Set a fixed length for padding/truncating\n",
        "fixed_length = 200\n",
        "\n",
        "# Pad sequences to the fixed length\n",
        "x_train_padded = pad_sequence(\n",
        "    [seq[:fixed_length] for seq in x_train],  # Truncate if longer\n",
        "    batch_first=True,\n",
        "    padding_value=0  # Use 0 for padding\n",
        ")\n",
        "\n",
        "x_test_padded = pad_sequence(\n",
        "    [seq[:fixed_length] for seq in x_test],\n",
        "    batch_first=True,\n",
        "    padding_value=0\n",
        ")\n",
        "\n",
        "y_train_padded = pad_sequence(\n",
        "    [torch.tensor(seq[:fixed_length]) for seq in y_train],\n",
        "    batch_first=True,\n",
        "    padding_value=0\n",
        ")\n",
        "\n",
        "y_test_padded = pad_sequence(\n",
        "    [torch.tensor(seq[:fixed_length]) for seq in y_test],\n",
        "    batch_first=True,\n",
        "    padding_value=0\n",
        ")\n",
        "\n",
        "# Convert y labels to tensors directly (assuming they are not sequences)\n",
        "#y_train_tensor = torch.tensor(y_train)  # Ensure it is a 1D tensor\n",
        "#y_test_tensor = torch.tensor(y_test)    # Ensure it is a 1D tensor\n",
        "\n",
        "# Checking the shapes of the prepared datasets\n",
        "print(x_train_padded.shape)\n",
        "print(y_train_padded.shape)\n",
        "print(x_test_padded.shape)\n",
        "print(y_test_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF9z5uEr-iIQ",
        "outputId": "db9f4a3d-afdd-4514-dfc1-e632dc1226ba"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([14006, 200])\n",
            "torch.Size([14006, 200])\n",
            "torch.Size([6003, 200])\n",
            "torch.Size([6003, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "UI6w-9efpcfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Design you RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        super(RNNModel, self).__init__()\n",
        "        # Embedding layer to learn a dense representation of the input bytes\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        # define some layers\n",
        "\n",
        "         # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Sigmoid activation for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding the input sequence\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Passing through the LSTM layer\n",
        "        lstm_out, (hn, cn) = self.lstm(x)\n",
        "\n",
        "        # Taking the output from the last time step (many-to-one)\n",
        "        lstm_out_last = lstm_out[:, -1, :]  # shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Passing through the fully connected layer\n",
        "        fc_out = self.fc(lstm_out_last)\n",
        "\n",
        "        # Applying sigmoid activation\n",
        "        out = self.sigmoid(fc_out)\n",
        "\n",
        "        return out\n",
        "    #def forward(self, x):\n",
        "        # forward process\n",
        "\n",
        "        #return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "S9h02mczpcfG"
      },
      "outputs": [],
      "source": [
        "# Define your loss, optimizer, and other hyper-parameters\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.BCELoss()  # Binary cross-entropy for binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seq_len = 200\n",
        "#model = RNNModel(seq_len).to(device)\n",
        "\n",
        "# Define the RNN model\n",
        "model = RNNModel(seq_len, vocab_size=256, embed_dim=128, hidden_dim=256, num_layers=2, output_dim=1).to(device)\n",
        "\n",
        "# Define loss function (binary cross-entropy)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define optimizer (Adam optimizer)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "7nYqy2VmpcfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bebcf3-774a-47a6-9fa6-648002fc9c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.0002207953245181542, Test Loss: 0.0002381456573327, Test Acc: 0.9975012493753124\n",
            "Epoch 2/10, Loss: 9.080215724067397e-05, Test Loss: 4.5113004164109494e-05, Test Acc: 0.9991670831251042\n",
            "Epoch 3/10, Loss: 2.3109445364795758e-05, Test Loss: 3.7202013615927146e-05, Test Acc: 0.9995002498750625\n",
            "Epoch 4/10, Loss: 1.600105870208751e-05, Test Loss: 3.768547315245874e-05, Test Acc: 0.9995002498750625\n",
            "Epoch 5/10, Loss: 1.042347121166559e-05, Test Loss: 3.459051451716018e-05, Test Acc: 0.9998334166250208\n",
            "Epoch 6/10, Loss: 1.2843809313828326e-05, Test Loss: 4.0087081728498824e-05, Test Acc: 0.9993336665000833\n",
            "Epoch 7/10, Loss: 1.3152772601546962e-05, Test Loss: 3.9512627486435435e-05, Test Acc: 0.9991670831251042\n",
            "Epoch 8/10, Loss: 9.097333365915057e-06, Test Loss: 3.632979087683282e-05, Test Acc: 0.9996668332500417\n",
            "Epoch 9/10, Loss: 7.230619895906689e-06, Test Loss: 3.6065181850077044e-05, Test Acc: 0.9996668332500417\n",
            "Epoch 10/10, Loss: 6.03172582349992e-06, Test Loss: 3.535818499386953e-05, Test Acc: 0.9998334166250208\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, x_train_padded.shape[0], batch_size):\n",
        "        # Getting batch data\n",
        "        batch_x = x_train_padded[i:i + batch_size].to(device)\n",
        "        batch_y = y_train_padded[i:i + batch_size, -1].to(device).float()  # Ensuring labels are float for BCELoss\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_x).squeeze()  # Squeezing for correct output shape for BCELoss\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backpropagate the loss and update model's parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Testing/Evaluation Phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, x_test_padded.shape[0], batch_size):\n",
        "            batch_test_x = x_test_padded[i:i + batch_size].to(device)\n",
        "            batch_test_y = y_test_padded[i:i + batch_size, -1].to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            test_outputs = model(batch_test_x).squeeze()\n",
        "\n",
        "            # Compute test loss\n",
        "            loss_test = criterion(test_outputs, batch_test_y)\n",
        "            test_loss += loss_test.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = (test_outputs >= 0.5).float()  # Classifying based on threshold 0.5\n",
        "            correct += (predictions == batch_test_y).sum().item()\n",
        "            total += batch_test_y.size(0)\n",
        "            accuracy = correct / total  # Calculate accuracy\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(x_train_padded)}, Test Loss: {test_loss / len(x_test_padded)}, Test Acc: {accuracy}\")\n",
        "\n",
        "\n",
        "# Also, don't forget to handle device assignments (to GPU or CPU) using .to(device) if you use GPU\n",
        "\n",
        "# Save model\n",
        "# torch.save(model.state_dict(), 'model_file.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "3U1biCRUpcfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221c9d69-4f23-4249-ea98-18a81fe484e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.98%\n",
            "Test Precision: 1.0000\n",
            "Test Recall: 0.9333\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the performance of your final model on test set using accuracy, precision and recall.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Variables for accumulating performance metrics\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Disable gradient calculations for evaluation\n",
        "with torch.no_grad():\n",
        "    for i in range(0, x_test_padded.shape[0], batch_size):\n",
        "        # Get the batch of test data\n",
        "        batch_test_x = x_test_padded[i:i + batch_size].to(device)\n",
        "        batch_test_y = y_test_padded[i:i + batch_size, -1].to(device).float()\n",
        "\n",
        "        # Forward pass\n",
        "        test_outputs = model(batch_test_x).squeeze()\n",
        "\n",
        "        # Convert model output to binary predictions (threshold 0.5 for binary classification)\n",
        "        predictions = (test_outputs >= 0.5).float()\n",
        "\n",
        "        # Append predictions and labels for evaluation\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(batch_test_y.cpu().numpy())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct += (predictions == batch_test_y).sum().item()\n",
        "        total += batch_test_y.size(0)\n",
        "\n",
        "# Convert predictions and labels to numpy arrays for metric calculations\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}